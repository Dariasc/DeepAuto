<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=vKAypyuXOvVpvQA8-joTiXTmHlnqyazravHt0oJcClIIKuHviXMj-TGTMIiZjPctBnIqpcFAcReH0Tw3JLCcnA');ol.lst-kix_k02b7r6g45wr-8.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-8 0}ol.lst-kix_k02b7r6g45wr-1.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-1 0}.lst-kix_k02b7r6g45wr-4>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-4}.lst-kix_xmkhvwsli27o-8>li:before{content:"\0025a0  "}.lst-kix_xmkhvwsli27o-2>li:before{content:"\0025a0  "}.lst-kix_xmkhvwsli27o-1>li:before{content:"\0025cb  "}.lst-kix_xmkhvwsli27o-0>li:before{content:"\0025cf  "}.lst-kix_k02b7r6g45wr-4>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-4,lower-latin) ". "}ol.lst-kix_k02b7r6g45wr-2.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-2 0}.lst-kix_k02b7r6g45wr-2>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-2,lower-roman) ". "}.lst-kix_k02b7r6g45wr-6>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-6,decimal) ". "}.lst-kix_k02b7r6g45wr-1>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-1,lower-latin) ". "}.lst-kix_k02b7r6g45wr-5>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-5,lower-roman) ". "}.lst-kix_k02b7r6g45wr-0>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-0,decimal) ". "}.lst-kix_k02b7r6g45wr-8>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-8,lower-roman) ". "}ul.lst-kix_xmkhvwsli27o-3{list-style-type:none}ul.lst-kix_xmkhvwsli27o-2{list-style-type:none}.lst-kix_k02b7r6g45wr-7>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-7,lower-latin) ". "}ul.lst-kix_xmkhvwsli27o-1{list-style-type:none}ol.lst-kix_k02b7r6g45wr-5.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-5 0}ul.lst-kix_xmkhvwsli27o-0{list-style-type:none}.lst-kix_k02b7r6g45wr-3>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-3}ul.lst-kix_xmkhvwsli27o-7{list-style-type:none}ul.lst-kix_xmkhvwsli27o-6{list-style-type:none}.lst-kix_xmkhvwsli27o-3>li:before{content:"\0025cf  "}ul.lst-kix_xmkhvwsli27o-5{list-style-type:none}ul.lst-kix_xmkhvwsli27o-4{list-style-type:none}.lst-kix_xmkhvwsli27o-4>li:before{content:"\0025cb  "}.lst-kix_k02b7r6g45wr-0>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-0}ul.lst-kix_xmkhvwsli27o-8{list-style-type:none}.lst-kix_xmkhvwsli27o-6>li:before{content:"\0025cf  "}.lst-kix_xmkhvwsli27o-5>li:before{content:"\0025a0  "}.lst-kix_xmkhvwsli27o-7>li:before{content:"\0025cb  "}.lst-kix_k02b7r6g45wr-3>li:before{content:"" counter(lst-ctn-kix_k02b7r6g45wr-3,decimal) ". "}.lst-kix_k02b7r6g45wr-6>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-6}.lst-kix_k02b7r6g45wr-1>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-1}.lst-kix_k02b7r6g45wr-7>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-7}ol.lst-kix_k02b7r6g45wr-6.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-6 0}ol.lst-kix_k02b7r6g45wr-3.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-3 0}.lst-kix_k02b7r6g45wr-5>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-5}.lst-kix_k02b7r6g45wr-8>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-8}ol.lst-kix_k02b7r6g45wr-0.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-0 0}.lst-kix_k02b7r6g45wr-2>li{counter-increment:lst-ctn-kix_k02b7r6g45wr-2}ol.lst-kix_k02b7r6g45wr-0{list-style-type:none}ol.lst-kix_k02b7r6g45wr-4.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-4 0}ol.lst-kix_k02b7r6g45wr-1{list-style-type:none}ol.lst-kix_k02b7r6g45wr-4{list-style-type:none}ol.lst-kix_k02b7r6g45wr-5{list-style-type:none}ol.lst-kix_k02b7r6g45wr-2{list-style-type:none}ol.lst-kix_k02b7r6g45wr-3{list-style-type:none}ol.lst-kix_k02b7r6g45wr-7.start{counter-reset:lst-ctn-kix_k02b7r6g45wr-7 0}ol.lst-kix_k02b7r6g45wr-8{list-style-type:none}ol.lst-kix_k02b7r6g45wr-6{list-style-type:none}ol.lst-kix_k02b7r6g45wr-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c0{padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:justify;height:11pt}.c6{padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c3{padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:justify}.c14{background-color:#fffafa;font-size:9.5pt;font-family:"Courier New";color:#333333;font-weight:400}.c4{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c9{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c11{font-weight:500;font-size:12pt;font-family:"Roboto"}.c2{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c21{font-weight:300;font-size:16pt;font-family:"Roboto"}.c1{font-size:12pt;font-family:"Roboto";font-weight:400}.c16{font-weight:100;font-size:12pt;font-family:"Roboto"}.c17{font-weight:400;font-size:11pt;font-family:"Arial"}.c19{font-size:12pt;font-family:"Roboto";font-weight:700}.c20{font-weight:500;font-size:26pt;font-family:"Roboto"}.c8{font-weight:300;font-size:12pt;font-family:"Roboto"}.c12{color:inherit;text-decoration:inherit}.c13{padding:0;margin:0}.c7{color:#222222;font-size:12pt}.c5{font-style:italic}.c18{padding-left:0pt}.c10{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c2"><div><p class="c15"><span class="c4 c17"></span></p></div><p class="c3"><span class="c4 c20">DeepAuto</span></p><p class="c3"><span class="c4 c21">Auto Conducci&oacute;n utilizando Redes Neuronales Profundas</span></p><p class="c3"><span class="c4 c16">Diego Arias</span></p><p class="c0"><span class="c4 c16"></span></p><p class="c3"><span class="c4 c1">La inteligencia artificial y el machine learning pueden resolver todo tipo de problemas, desde la identificaci&oacute;n de im&aacute;genes, hasta decidir que pel&iacute;cula deber&iacute;as ver. La inteligencia artificial ya se est&aacute; aplicando en todo tipo de cosas y una de esas es en el campo de desarrollo de veh&iacute;culos, compa&ntilde;&iacute;as como; Tesla, Waymo (Google) y compa&ntilde;&iacute;as de autos promocionando proyectos existentes como BMW con Nvidia, est&aacute;n impulsando el desarrollo de veh&iacute;culos que se conducen solos.</span></p><p class="c3"><span class="c4 c1">Estos entrenan con miles de fotos y estad&iacute;sticas sobre la conducci&oacute;n de los seres humanos y estas la utilizan los autom&oacute;viles autom&aacute;ticos para simular la conducci&oacute;n humana.</span></p><p class="c3"><span class="c1">Todos estos son proyectos propietarios. La idea de este proyecto es que cualquier persona que tenga un computador (Dentro de cierto requisito) pueda descargar el c&oacute;digo fuente, correrlo, entenderlo, construir en base a &eacute;l y subir cambios </span><span class="c1 c5">upstream</span><span class="c4 c1">. Hacer de esto un proyecto de educaci&oacute;n con buena documentaci&oacute;n es esencial para que cumpla su prop&oacute;sito, as&iacute; personas pueden entender como funcionan estos autos que gradualmente van a reemplazar al conductor humano. Y qu&eacute; mecanismos est&aacute;n ah&iacute; para evitar accidentes.</span></p><p class="c3"><span class="c4 c1">Proyectos as&iacute; siempre presentan problemas &eacute;ticos, estos proyectos en un futuro pueden reemplazar la necesidad que un conductor de cami&oacute;n tenga que hacer su trabajo, o funcionarios del transporte p&uacute;blico tengan que manejar los buses, esto es una realidad lejana pero est&aacute; ah&iacute;. </span></p><p class="c0"><span class="c4 c1"></span></p><p class="c3"><span class="c1">El proyecto se basa en crear un auto que se conduce &ldquo;solo&rdquo; utilizando </span><span class="c1 c5">Python</span><span class="c1">&nbsp;para el desarrollo de las </span><span class="c1 c5">Redes</span><span class="c1">&nbsp;y el manejo del </span><span class="c1 c5">Training Data</span><span class="c1">, Unity para el &ldquo;ambiente&rdquo; donde esta </span><span class="c1 c5">Red</span><span class="c1">&nbsp;entrena y aprende a simular. Tambi&eacute;n uno de los objetivos del proyecto fue hacerlo </span><span class="c1 c5">Open Source</span><span class="c1">&nbsp;y se puede encontrar en </span><span class="c8">github.com/Dariasc/DeepAuto</span><span class="c1">&nbsp;tiene una licensia GPL3</span><span class="c4 c1">.</span></p><p class="c3"><span class="c4 c1">Unity es usualmente utilizado para crear videojuegos pero para este caso tiene todo lo necesario para crear un ambiente virtual donde esta red pueda entrenar. Unity incluye un demo de autos que se utiliza en el desarrollo de esta Red. Incluye una pista y varios autom&oacute;viles para que aprenda c&oacute;mo manejar.</span></p><p class="c0"><span class="c4 c1"></span></p><p class="c0"><span class="c4 c1"></span></p><p class="c3"><span class="c4 c1">Para v0.1 la idea era simple: Imagen frontal desde el punto de vista del parachoque es todo lo que la red neuronal va a tener para manejar en esta pista. Estamos utilizando un auto con conducci&oacute;n frontal para esta versi&oacute;n pero gracias a unity estos hiper par&aacute;metros pueden ser modificados.</span></p><p class="c3"><span class="c1">Unity saca la foto y la codifica en </span><span class="c1 c5">base64 </span><span class="c4 c1">b&aacute;sicamente convierte los bits de la imagen en un String ASCII (American Standard Code for Information Interchange) lo cual es enviado como telemetr&iacute;a a un servidor Python de sockets este manipula los datos como quiera, ya sea predecir qu&eacute; debe hacer el auto ahora, o guardar la imagen para entrenamiento.</span></p><p class="c3"><span class="c4 c1">La red neuronal es convolucional (Buena identificando patrones en im&aacute;genes) y esta termina en una capa densa final de dos neuronas que pueden ser -1 hasta 1. Primera neurona define la direcci&oacute;n, -1 quebrar Izquierda, 1 quebrar Derecha o 0 seguir derecho (Pueden ser decimales, y probablemente son). La otra neurona define velocidad, -1 Retroceder o Frenar, 1 Acelerar al m&aacute;ximo, 0 No aplicar nada. </span></p><p class="c3"><span class="c4 c1">Se modific&oacute; un modelo utilizado por Nvidia.</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 619.00px; height: 213.00px;"><img alt="" src="images/image2.png" style="width: 619.00px; height: 213.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c4 c1">Con un training data de 10k im&aacute;genes con indicaciones de direccion la red empez&oacute; a entender que es lo que deb&iacute;a hacer, los primeros 10 segundos de la pista iba bien hasta que llegaba un punto donde ten&iacute;a que doblar o hacer algo brusco perd&iacute;a todo sentido, esto probablemente se atribuy&oacute; a la falta de training data, y con eso en mente se entren&oacute; con a&uacute;n m&aacute;s data.</span></p><p class="c3"><span class="c4 c1">Con un training data de 120k el auto logr&oacute; conducir exitosamente alrededor de &#8532; de las ejecuciones. Pero esta red ten&iacute;a algunos problemas que se identifican cuando uno corre esta red:</span></p><p class="c3"><span class="c4 c11">Sentido del Tiempo</span></p><p class="c3"><span class="c1">La red neuronal solo ten&iacute;a una imagen del parachoque del auto sin entender ni movimiento, ni tiempo, ni velocidad. Entonces en algunos casos cuando ten&iacute;a que doblar como la velocidad y freno/reversa est&aacute;n en el vector un dimensional suced&iacute;a que cuando el auto entraba a una situaci&oacute;n en la que no se a enfrentado y tiene que doblar no entiende su velocidad. Entonces suced&iacute;a que si </span><span class="c19">A:</span><span class="c1">&nbsp;Velocidad es baja el auto frenaba tanto que empezaba a retroceder </span><span class="c19">B:</span><span class="c4 c1">&nbsp;Velocidad muy alta el auto no frena lo suficiente y terminaba estrell&aacute;ndose.</span></p><p class="c3"><span class="c4 c11">Overfitting</span></p><p class="c3"><span class="c4 c1">Con la cantidad de informaci&oacute;n con la cual fue entrenada la red y el tiempo por lo que se entren&oacute; probablemente sucedi&oacute; algo llamado overfitting donde la red aprendi&oacute; a manejarse dentro de su dataset muy bien pero enfrentarlo con otro ambiente (La pista al rev&eacute;s o otra pista) lo llevar&iacute;a a fallar.</span></p><p class="c3"><span class="c4 c11">Puntos Ciegos</span></p><p class="c3"><span class="c4 c1">El auto no tiene &ldquo;ojos&rdquo; a sus lados, cualquier intento de ponerlo en un ambiente menos amigable como una ciudad lo llevar&iacute;a a fallar por varias razones pero principalmente por no entender nada a sus lados.</span></p><p class="c0"><span class="c4 c1"></span></p><p class="c3"><span class="c4 c1">Con estas consideraciones se comenz&oacute; el desarrollo de una v0.2 y se propusieron las siguientes soluciones de los problemas anteriores:</span></p><p class="c3"><span class="c4 c11">Sentido del Tiempo</span></p><p class="c3"><span class="c4 c1">Hay 3 principales soluciones para este problema:</span></p><ol class="c13 lst-kix_k02b7r6g45wr-0 start" start="1"><li class="c3 c10 c18"><span class="c4 c1">Pasarle las ultimas 2 imagenes y la actual para que as&iacute; pueda entender su velocidad con la diferencia entre las 3 im&aacute;genes. Esto ser&iacute;a como pasarle una imagen RGB (3 Canales) con el solo beneficio de ver la diferencia entre las im&aacute;genes, el v0.1 puede procesar 22 fotogramas por segundo, agregarle 2 capas m&aacute;s haria que disminuya su rendimiento en &#8531; de su rendimiento actual (7 fotogramas, estimaci&oacute;n probablemente mas fotogramas) este rendimiento se puede correr con una mejor GPU pero dejar&iacute;a el l&iacute;mite de entrada al desarrollo a&uacute;n m&aacute;s alto.</span></li><li class="c3 c18 c10"><span class="c4 c1">Codificar la velocidad en la imagen en una linea de pixeles como una barra de progreso.</span></li><li class="c3 c18 c10"><span class="c1">Concatenar la informaci&oacute;n adicional a la red convolucional utilizando un objeto de la librer&iacute;a de Keras llamado </span><span class="c14">keras.layers.Concatenate()</span><span class="c1">&nbsp;b&aacute;sicamente junta dos capas para que produzcan hacia una sola capa. La velocidad seria: </span><span class="c1 c5">Velocidad / Velocidad M&aacute;xima</span><span class="c4 c1">, para as&iacute; normalizar el data.</span></li></ol><p class="c3 c10"><span class="c1">Ver: </span><span class="c9 c1"><a class="c12" href="https://www.google.com/url?q=https://keras.io/getting-started/functional-api-guide/%23multi-input-and-multi-output-models&amp;sa=D&amp;ust=1536098869012000">https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models</a></span><span class="c4 c1">&nbsp;</span></p><p class="c0"><span class="c4 c11"></span></p><p class="c3"><span class="c4 c11">Overfitting</span></p><p class="c3"><span class="c4 c1">Para solucionar overfitting la idea es separar el vector de aceleraci&oacute;n y freno/reserva a 3 distintos o separar reverse y freno a distinto como, [aceleraci&oacute;n/reversa, freno] o [aceleraci&oacute;n, reversa, freno] para que la velocidad no influye si es que el auto frena o empieza a retroceder.</span></p><p class="c3"><span class="c4 c11">Puntos Ciegos</span></p><p class="c3"><span class="c1">Para satisfacer la futura necesidad de ver por los puntos ciegos se implement&oacute; un sistema de sensores utilizando </span><span class="c1 c5">raycasts </span><span class="c1">son modificables en t&eacute;rminos de distancia direcci&oacute;n y punto de inicio, esta ser&aacute; entregada al programa python como </span><span class="c1 c5">telemetr&iacute;a</span><span class="c1">&nbsp;adicional. Y ser&iacute;a utilizado en conjunto con la velocidad como una capa adicional. El n&uacute;mero entregado seria; </span><span class="c1 c5">Distancia Actual / Distancia M&aacute;xima</span><span class="c4 c1">&nbsp;para normalizar el data.</span></p><p class="c0"><span class="c4 c1"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 468.00px; height: 262.45px;"><img alt="" src="images/image3.gif" style="width: 472.54px; height: 262.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c1">Otra de las decisiones que se tom&oacute; fue la incrementaci&oacute;n de la resoluci&oacute;n de 80x120, a 120x180 para que identifique obst&aacute;culos a distancia como los </span><span class="c7">guardarrail que a distancia son dif&iacute;ciles de identificar</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 619.00px; height: 405.69px;"><img alt="" src="images/image1.png" style="width: 619.00px; height: 405.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c4 c8"></span></p><p class="c0"><span class="c4 c8"></span></p><p class="c0"><span class="c4 c8"></span></p><p class="c0"><span class="c4 c8"></span></p><p class="c3"><span class="c4 c8">Bibliografia:</span></p><p class="c3"><span class="c8">Paper NVIDIA, &ldquo;End to End Deep Learning&rdquo; </span><span class="c1 c9"><a class="c12" href="https://www.google.com/url?q=https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf&amp;sa=D&amp;ust=1536098869014000">https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf</a></span></p><p class="c0"><span class="c4 c1"></span></p></body></html>